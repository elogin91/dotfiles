"use strict";

exports.__esModule = true;
exports.MULTIPART_THRESHOLD = exports.COMPRESS_THRESHOLD = void 0;
exports.checkFileAccess = checkFileAccess;
exports.detectCompressedMimeType = detectCompressedMimeType;
exports.getFileHash = void 0;
exports.getFileMeta = getFileMeta;
exports.getFileSize = getFileSize;
exports.getPartBoundaries = getPartBoundaries;
exports.isFile = isFile;
exports.unzipFile = void 0;
exports.uploadImportSqlFileToS3 = uploadImportSqlFileToS3;
exports.uploadParts = uploadParts;
var _chalk = _interopRequireDefault(require("chalk"));
var _crypto = require("crypto");
var _debug = _interopRequireDefault(require("debug"));
var _fs = require("fs");
var _nodeFetch = _interopRequireDefault(require("node-fetch"));
var _promises = require("node:fs/promises");
var _promises2 = require("node:stream/promises");
var _os = _interopRequireDefault(require("os"));
var _path = _interopRequireDefault(require("path"));
var _stream = require("stream");
var _xml2js = require("xml2js");
var _zlib = require("zlib");
var _http = _interopRequireDefault(require("../lib/api/http"));
var _fileSize = require("../lib/constants/file-size");
function _interopRequireDefault(e) { return e && e.__esModule ? e : { default: e }; }
// Need to use CommonJS imports here as the `fetch-retry` typedefs are messed up and throwing TypeJS errors when using `import`
// eslint-disable-next-line @typescript-eslint/no-unsafe-assignment
const fetchWithRetry =
// eslint-disable-next-line @typescript-eslint/no-unsafe-call, @typescript-eslint/no-var-requires
require('fetch-retry')(_nodeFetch.default, {
  // Set default retry options
  retries: 3,
  retryDelay: attempt => {
    return Math.pow(2, attempt) * 1000; // 1000, 2000, 4000
  }
});
const debug = (0, _debug.default)('vip:lib/client-file-uploader');

// Files smaller than COMPRESS_THRESHOLD will not be compressed before upload
const COMPRESS_THRESHOLD = exports.COMPRESS_THRESHOLD = 16 * _fileSize.MB_IN_BYTES;

// Files smaller than MULTIPART_THRESHOLD will use `PutObject` vs Multipart Uploads
const MULTIPART_THRESHOLD = exports.MULTIPART_THRESHOLD = 32 * _fileSize.MB_IN_BYTES;

// This is how big each part of a Multipart Upload is (except the last / remainder)
const UPLOAD_PART_SIZE = 16 * _fileSize.MB_IN_BYTES;

// How many parts will upload at the same time
const MAX_CONCURRENT_PART_UPLOADS = 5;

// TODO: Replace with a proper definitions once we convert lib/cli/command.js to TypeScript

const getWorkingTempDir = () => (0, _promises.mkdtemp)(_path.default.join(_os.default.tmpdir(), 'vip-client-file-uploader'));
const getFileHash = async (fileName, hashType = 'md5') => {
  const src = (0, _fs.createReadStream)(fileName);
  const dst = (0, _crypto.createHash)(hashType);
  try {
    await (0, _promises2.pipeline)(src, dst);
    return dst.digest().toString('hex');
  } catch (err) {
    throw new Error(`Could not generate file hash: ${err.message}`, {
      cause: err
    });
  } finally {
    src.close();
  }
};
exports.getFileHash = getFileHash;
const gzipFile = async (uncompressedFileName, compressedFileName) => {
  try {
    await (0, _promises2.pipeline)((0, _fs.createReadStream)(uncompressedFileName), (0, _zlib.createGzip)(), (0, _fs.createWriteStream)(compressedFileName));
  } catch (err) {
    throw new Error(`Could not compress file: ${err.message}`, {
      cause: err
    });
  }
};

/**
 * Extract a .gz file and save it to a specified location
 *
 * @param {string} inputFilename  The file to unzip
 * @param {string} outputFilename The file where the unzipped data will be written
 * @return {Promise} A promise that resolves when the file is unzipped
 */
const unzipFile = async (inputFilename, outputFilename) => {
  const mimeType = await detectCompressedMimeType(inputFilename);
  const extractFunctions = {
    'application/gzip': {
      extractor: _zlib.createGunzip
    }
  };
  const extractionInfo = extractFunctions[mimeType];

  // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition
  if (!extractionInfo) {
    throw new Error(`unsupported file format: ${mimeType}`);
  }
  const source = (0, _fs.createReadStream)(inputFilename);
  const destination = (0, _fs.createWriteStream)(outputFilename);
  await (0, _promises2.pipeline)(source, extractionInfo.extractor(), destination);
};
exports.unzipFile = unzipFile;
async function getFileMeta(fileName) {
  const fileSize = await getFileSize(fileName);
  const basename = _path.default.basename(fileName);
  // TODO Validate File basename...  encodeURIComponent, maybe...?

  const mimeType = await detectCompressedMimeType(fileName);
  // TODO Only allow a subset of Mime Types...?

  const isCompressed = ['application/zip', 'application/gzip'].includes(mimeType);
  return {
    basename,
    fileName,
    fileSize,
    isCompressed
  };
}
async function uploadImportSqlFileToS3({
  app,
  env,
  fileMeta,
  progressCallback,
  hashType = 'md5'
}) {
  let tmpDir;
  try {
    tmpDir = await getWorkingTempDir();
  } catch (err) {
    throw new Error(`Unable to create temporary working directory: ${err.message}`, {
      cause: err
    });
  }
  debug(`File ${_chalk.default.cyan(fileMeta.basename)} is ~ ${Math.floor(fileMeta.fileSize / _fileSize.MB_IN_BYTES)} MB\n`);

  // TODO Compression will probably fail over a certain file size... break into pieces...?
  // TODO if needed add a flag to bypass auto-compression

  if (!fileMeta.isCompressed && fileMeta.fileSize >= COMPRESS_THRESHOLD) {
    // Compress to the temp dir & annotate `fileMeta`
    const uncompressedFileName = fileMeta.fileName;
    const uncompressedFileSize = fileMeta.fileSize;
    fileMeta.basename = fileMeta.basename.replace(/(.gz)?$/i, '.gz');
    fileMeta.fileName = _path.default.join(tmpDir, fileMeta.basename);
    debug(`Compressing the file to ${_chalk.default.cyan(fileMeta.fileName)} prior to transfer...`);
    await gzipFile(uncompressedFileName, fileMeta.fileName);
    fileMeta.isCompressed = true;
    fileMeta.fileSize = await getFileSize(fileMeta.fileName);
    debug(`Compressed file is ~ ${Math.floor(fileMeta.fileSize / _fileSize.MB_IN_BYTES)} MB\n`);
    const fewerBytes = uncompressedFileSize - fileMeta.fileSize;
    const calculation = `${(fewerBytes / _fileSize.MB_IN_BYTES).toFixed(2)}MB (${Math.floor(100 * fewerBytes / uncompressedFileSize)}%)`;
    debug(`** Compression resulted in a ${calculation} smaller file ðŸ“¦ **\n`);
  }
  debug(`Calculating file ${hashType} checksum...`);
  const checksum = await getFileHash(fileMeta.fileName, hashType);
  debug(`Calculated file ${hashType} checksum: ${checksum}\n`);
  const result = fileMeta.fileSize < MULTIPART_THRESHOLD ? await uploadUsingPutObject({
    app,
    env,
    fileMeta,
    progressCallback
  }) : await uploadUsingMultipart({
    app,
    env,
    fileMeta,
    progressCallback
  });
  return {
    fileMeta,
    checksum,
    result
  };
}

/**
 * @see https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html#API_CompleteMultipartUpload_Example_3
 */

async function uploadUsingPutObject({
  app,
  env,
  fileMeta: {
    basename,
    fileContent,
    fileName,
    fileSize
  },
  progressCallback
}) {
  debug(`Uploading ${_chalk.default.cyan(basename)} to S3 using the \`PutObject\` command`);
  const presignedRequest = await getSignedUploadRequestData({
    appId: app.id,
    envId: env.id,
    basename,
    action: 'PutObject'
  });
  const fetchOptions = presignedRequest.options;
  fetchOptions.headers = {
    ...fetchOptions.headers,
    'Content-Length': `${fileSize}` // This has to be a string
  };
  let readBytes = 0;
  const progressPassThrough = new _stream.PassThrough();
  progressPassThrough.on('data', data => {
    readBytes += data.length;
    const percentage = `${Math.floor(100 * readBytes / fileSize)}%`;
    debug(percentage);
    if (typeof progressCallback === 'function') {
      progressCallback(percentage);
    }
  });
  const response = await fetchWithRetry(presignedRequest.url, {
    ...fetchOptions,
    body: fileContent ?? (0, _fs.createReadStream)(fileName).pipe(progressPassThrough)
  });
  if (response.status === 200) {
    return 'ok';
  }
  const result = await response.text();

  // TODO is any additional hardening needed here?
  const parser = new _xml2js.Parser({
    explicitArray: false,
    ignoreAttrs: true
  });
  let parsedResponse;
  try {
    parsedResponse = await parser.parseStringPromise(result);
  } catch (err) {
    throw new Error(`Invalid response from cloud service. ${err.message}`, {
      cause: err
    });
  }
  const {
    Code,
    Message
  } = parsedResponse.Error;
  throw new Error(`Unable to upload to cloud storage. ${JSON.stringify({
    Code,
    Message
  })}`);
}

/**
 * @see https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html#API_CreateMultipartUpload_ResponseSyntax
 */

async function uploadUsingMultipart({
  app,
  env,
  fileMeta,
  progressCallback
}) {
  const {
    basename
  } = fileMeta;
  debug(`Uploading ${_chalk.default.cyan(basename)} to S3 using the Multipart API.`);
  const presignedCreateMultipartUpload = await getSignedUploadRequestData({
    appId: app.id,
    envId: env.id,
    basename,
    action: 'CreateMultipartUpload'
  });
  const multipartUploadResponse = await fetchWithRetry(presignedCreateMultipartUpload.url, presignedCreateMultipartUpload.options);
  const multipartUploadResult = await multipartUploadResponse.text();

  // TODO is any hardening needed here?
  const parser = new _xml2js.Parser({
    explicitArray: false,
    ignoreAttrs: true
  });
  const parsedResponse = await parser.parseStringPromise(multipartUploadResult);
  if ('Error' in parsedResponse) {
    const {
      Code,
      Message
    } = parsedResponse.Error;
    throw new Error(`Unable to create cloud storage object. Error: ${JSON.stringify({
      Code,
      Message
    })}`);
  }
  if (!('InitiateMultipartUploadResult' in parsedResponse) || !parsedResponse.InitiateMultipartUploadResult.UploadId) {
    throw new Error(`Unable to get Upload ID from cloud storage. Error: ${multipartUploadResult}`);
  }
  const uploadId = parsedResponse.InitiateMultipartUploadResult.UploadId;
  debug({
    uploadId
  });
  const parts = getPartBoundaries(fileMeta.fileSize);
  const etagResults = await uploadParts({
    app,
    env,
    fileMeta,
    parts,
    uploadId,
    progressCallback
  });
  debug({
    etagResults
  });
  return completeMultipartUpload({
    app,
    env,
    basename,
    uploadId,
    etagResults
  });
}
async function getSignedUploadRequestData({
  action,
  appId,
  basename,
  envId,
  etagResults,
  uploadId = undefined,
  partNumber = undefined
}) {
  const WPVIP_DEPLOY_TOKEN = process.env.WPVIP_DEPLOY_TOKEN;
  const reqOptions = {
    method: 'POST',
    body: {
      action,
      appId,
      basename,
      envId,
      etagResults,
      partNumber,
      uploadId
    }
  };
  if (WPVIP_DEPLOY_TOKEN) {
    reqOptions.headers = {
      Authorization: `Bearer ${WPVIP_DEPLOY_TOKEN}`
    };
  }
  const response = await (0, _http.default)('/upload/site-import-presigned-url', reqOptions);
  if (response.status !== 200) {
    throw new Error((await response.text()) || response.statusText);
  }
  return response.json();
}
async function checkFileAccess(fileName) {
  return (0, _promises.access)(fileName, _fs.constants.R_OK);
}
async function isFile(fileName) {
  try {
    const stats = await (0, _promises.stat)(fileName);
    return stats.isFile();
  } catch (err) {
    debug(`isFile error: ${err.message}`);
    return false;
  }
}
async function getFileSize(fileName) {
  const stats = await (0, _promises.stat)(fileName);
  return stats.size;
}
async function detectCompressedMimeType(fileName) {
  const ZIP_MAGIC_NUMBER = '504b0304';
  const GZ_MAGIC_NUMBER = '1f8b';
  const file = await (0, _promises.open)(fileName, 'r');
  const {
    buffer
  } = await file.read(Buffer.alloc(4), 0, 4, 0);
  const fileHeader = buffer.toString('hex');
  await file.close();
  if (ZIP_MAGIC_NUMBER === fileHeader.slice(0, ZIP_MAGIC_NUMBER.length)) {
    return 'application/zip';
  }
  if (GZ_MAGIC_NUMBER === fileHeader.slice(0, GZ_MAGIC_NUMBER.length)) {
    return 'application/gzip';
  }
  return '';
}
function getPartBoundaries(fileSize) {
  if (fileSize < 1) {
    throw new Error('fileSize must be greater than zero');
  }
  const numParts = Math.ceil(fileSize / UPLOAD_PART_SIZE);
  return new Array(numParts).fill(undefined).map((_numPart, index) => {
    const start = index * UPLOAD_PART_SIZE;
    const remaining = fileSize - start;
    const end = (remaining > UPLOAD_PART_SIZE ? start + UPLOAD_PART_SIZE : start + remaining) - 1;
    const partSize = end + 1 - start;
    return {
      end,
      index,
      partSize,
      start
    };
  });
}
async function uploadParts({
  app,
  env,
  fileMeta,
  uploadId,
  parts,
  progressCallback
}) {
  let uploadsInProgress = 0;
  let totalBytesRead = 0;
  const partPercentages = new Array(parts.length).fill(0);
  const readyForPartUpload = () => new Promise(resolve => {
    const canDoInterval = setInterval(() => {
      if (uploadsInProgress < MAX_CONCURRENT_PART_UPLOADS) {
        uploadsInProgress++;
        clearInterval(canDoInterval);
        resolve();
      }
    }, 300);
  });
  const updateProgress = () => {
    const percentage = `${Math.floor(100 * totalBytesRead / fileMeta.fileSize)}%`;
    if (typeof progressCallback === 'function') {
      progressCallback(percentage);
    }
    debug(partPercentages.map((partPercentage, index) => {
      const {
        partSize
      } = parts[index];
      return `Part # ${index}: ${partPercentage}% of ${(partSize / _fileSize.MB_IN_BYTES).toFixed(2)}MB`;
    }).join('\n') + `\n\nOverall Progress: ${percentage}% of ${(fileMeta.fileSize / _fileSize.MB_IN_BYTES).toFixed(2)}MB`);
  };
  const updateProgressInterval = setInterval(updateProgress, 500);
  const allDone = await Promise.all(parts.map(async part => {
    const {
      index,
      partSize
    } = part;
    const progressPassThrough = new _stream.PassThrough();
    let partBytesRead = 0;
    progressPassThrough.on('data', data => {
      totalBytesRead += data.length;
      partBytesRead += data.length;
      partPercentages[index] = Math.floor(100 * partBytesRead / partSize);
    });
    await readyForPartUpload();
    const uploadResult = await uploadPart({
      app,
      env,
      fileMeta,
      part,
      progressPassThrough,
      uploadId
    });
    uploadsInProgress--;
    return uploadResult;
  }));
  clearInterval(updateProgressInterval);
  updateProgress();
  return allDone;
}
async function uploadPart({
  app,
  env,
  fileMeta: {
    basename,
    fileName
  },
  part,
  progressPassThrough,
  uploadId
}) {
  const {
    end,
    index,
    partSize,
    start
  } = part;
  const s3PartNumber = index + 1; // S3 multipart is indexed from 1

  // TODO: handle failures / retries, etc.
  const doUpload = async () => {
    // Get the signed request data from Parker
    const partUploadRequestData = await getSignedUploadRequestData({
      action: 'UploadPart',
      appId: app.id,
      envId: env.id,
      basename,
      partNumber: s3PartNumber,
      uploadId
    });
    const fetchOptions = partUploadRequestData.options;
    fetchOptions.headers = {
      ...fetchOptions.headers,
      'Content-Length': `${partSize}` // This has to be a string
      /**
       * TODO? 'Content-MD5': Buffer.from( ... ).toString( 'base64' ),
       * Content-MD5 has to be base64 encoded.
       * It's the hash of the entire request object & has to be included in the signature,
       *   ...so it may not be feasible to include with presigned requests.
       */
    };
    fetchOptions.body = (0, _fs.createReadStream)(fileName, {
      start,
      end
    }).pipe(progressPassThrough);
    const fetchResponse = await fetchWithRetry(partUploadRequestData.url, fetchOptions);
    if (fetchResponse.status === 200) {
      const responseHeaders = fetchResponse.headers.raw();
      const [etag] = responseHeaders.etag;
      return JSON.parse(etag);
    }
    const result = await fetchResponse.text();

    // TODO is any hardening needed here?
    const parser = new _xml2js.Parser({
      explicitArray: false,
      ignoreAttrs: true
    });
    const parsed = await parser.parseStringPromise(result);
    const {
      Code,
      Message
    } = parsed.Error;
    throw new Error(`Unable to upload file part. Error: ${JSON.stringify({
      Code,
      Message
    })}`);
  };
  return {
    ETag: await doUpload(),
    PartNumber: s3PartNumber
  };
}

/**
 * @see https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html#API_CompleteMultipartUpload_ResponseSyntax
 */

async function completeMultipartUpload({
  app,
  env,
  basename,
  uploadId,
  etagResults
}) {
  const completeMultipartUploadRequestData = await getSignedUploadRequestData({
    action: 'CompleteMultipartUpload',
    appId: app.id,
    envId: env.id,
    basename,
    uploadId,
    etagResults
  });
  const completeMultipartUploadResponse = await fetchWithRetry(completeMultipartUploadRequestData.url, completeMultipartUploadRequestData.options);
  if (completeMultipartUploadResponse.status !== 200) {
    throw await completeMultipartUploadResponse.text();
  }

  /**
   * Processing of a Complete Multipart Upload request could take several minutes to complete.
   * After Amazon S3 begins processing the request, it sends an HTTP response header that specifies a 200 OK response.
   * While processing is in progress, Amazon S3 periodically sends white space characters to keep the connection from timing out.
   * Because a request could fail after the initial 200 OK response has been sent, it is important that you check the
   * response body to determine whether the request succeeded.
   * Note that if CompleteMultipartUpload fails, applications should be prepared to retry the failed requests.
   *
   * https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html
   */
  const result = await completeMultipartUploadResponse.text();
  const parser = new _xml2js.Parser({
    explicitArray: false,
    ignoreAttrs: true
  });
  const parsed = await parser.parseStringPromise(result);
  if ('Error' in parsed) {
    const {
      Code,
      Message
    } = parsed.Error;
    throw new Error(`Unable to complete the upload. Error: ${JSON.stringify({
      Code,
      Message
    })}`);
  }
  return parsed;
}